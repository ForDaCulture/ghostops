{
  "manifest_version": 3,
  "name": "ðŸ§  Prompt Poison Shield",
  "version": "1.0.0",
  "description": "Warns you before you submit sensitive data to Large Language Models (LLMs).",
  "permissions": [
    "storage",
    "activeTab",
    "scripting"
  ],
  "host_permissions": [
    "*://chat.openai.com/*",
    "*://claude.ai/*",
    "*://gemini.google.com/*",
    "*://bard.google.com/*",
    "*://perplexity.ai/*",
    "*://poe.com/*"
  ],
  "background": {
    "service_worker": "background.js"
  },
  "content_scripts": [
    {
      "matches": [
        "*://chat.openai.com/*",
        "*://claude.ai/*",
        "*://gemini.google.com/*",
        "*://bard.google.com/*",
        "*://perplexity.ai/*",
        "*://poe.com/*"
      ],
      "js": ["content.js"],
      "css": ["warning.css"]
    }
  ],
  "action": {
    "default_popup": "warning.html",
    "default_icon": {
      "16": "icons/icon16.png",
      "48": "icons/icon48.png",
      "128": "icons/icon128.png"
    }
  },
  "icons": {
    "16": "icons/icon16.png",
    "48": "icons/icon48.png",
    "128": "icons/icon128.png"
  },
  "web_accessible_resources": [
    {
      "resources": ["warning.html", "warning.css"],
      "matches": ["<all_urls>"]
    }
  ]
}
